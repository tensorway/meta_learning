{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bit41770f51cb494085b126429b02db281f",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Using cuda device\nWrapping the env in a DummyVecEnv.\n-----------------------------\n| time/              |      |\n|    fps             | 631  |\n|    iterations      | 1    |\n|    time_elapsed    | 3    |\n|    total_timesteps | 2048 |\n-----------------------------\n-----------------------------------------\n| time/                   |             |\n|    fps                  | 396         |\n|    iterations           | 2           |\n|    time_elapsed         | 10          |\n|    total_timesteps      | 4096        |\n| train/                  |             |\n|    approx_kl            | 0.008806784 |\n|    clip_fraction        | 0.234       |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -0.685      |\n|    explained_variance   | -1.47e+04   |\n|    learning_rate        | 0.0003      |\n|    loss                 | 9.7         |\n|    n_updates            | 10          |\n|    policy_gradient_loss | -0.0176     |\n|    value_loss           | 53.9        |\n-----------------------------------------\n---------------------------------------\n| time/                   |           |\n|    fps                  | 340       |\n|    iterations           | 3         |\n|    time_elapsed         | 18        |\n|    total_timesteps      | 6144      |\n| train/                  |           |\n|    approx_kl            | 0.0080545 |\n|    clip_fraction        | 0.0781    |\n|    clip_range           | 0.2       |\n|    entropy_loss         | -0.666    |\n|    explained_variance   | -80.9     |\n|    learning_rate        | 0.0003    |\n|    loss                 | 21.4      |\n|    n_updates            | 20        |\n|    policy_gradient_loss | -0.0147   |\n|    value_loss           | 43.5      |\n---------------------------------------\n-----------------------------------------\n| time/                   |             |\n|    fps                  | 332         |\n|    iterations           | 4           |\n|    time_elapsed         | 24          |\n|    total_timesteps      | 8192        |\n| train/                  |             |\n|    approx_kl            | 0.005875084 |\n|    clip_fraction        | 0.0938      |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -0.64       |\n|    explained_variance   | -11.2       |\n|    learning_rate        | 0.0003      |\n|    loss                 | 24.1        |\n|    n_updates            | 30          |\n|    policy_gradient_loss | -0.0157     |\n|    value_loss           | 62.3        |\n-----------------------------------------\n------------------------------------------\n| time/                   |              |\n|    fps                  | 320          |\n|    iterations           | 5            |\n|    time_elapsed         | 31           |\n|    total_timesteps      | 10240        |\n| train/                  |              |\n|    approx_kl            | 0.0041431496 |\n|    clip_fraction        | 0.0156       |\n|    clip_range           | 0.2          |\n|    entropy_loss         | -0.617       |\n|    explained_variance   | -6.58        |\n|    learning_rate        | 0.0003       |\n|    loss                 | 33.1         |\n|    n_updates            | 40           |\n|    policy_gradient_loss | -0.0119      |\n|    value_loss           | 71.8         |\n------------------------------------------\n"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "obs = env.reset()\n",
    "for i in range(1000):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "      obs = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Help on package stable_baselines3:\n\nNAME\n    stable_baselines3\n\nPACKAGE CONTENTS\n    a2c (package)\n    common (package)\n    ddpg (package)\n    dqn (package)\n    ppo (package)\n    sac (package)\n    stable_baselines3 (package)\n    td3 (package)\n\nDATA\n    file_handler = <_io.TextIOWrapper name='/home/darijan/.local/li...asel...\n    version_file = '/home/darijan/.local/lib/python3.8/site-packages/stabl...\n\nVERSION\n    0.7.0\n\nFILE\n    /home/darijan/.local/lib/python3.8/site-packages/stable_baselines3/__init__.py\n\n\nHelp on module stable_baselines3.common.on_policy_algorithm in stable_baselines3.common:\n\nNAME\n    stable_baselines3.common.on_policy_algorithm\n\nCLASSES\n    stable_baselines3.common.base_class.BaseAlgorithm(abc.ABC)\n        OnPolicyAlgorithm\n    \n    class OnPolicyAlgorithm(stable_baselines3.common.base_class.BaseAlgorithm)\n     |  OnPolicyAlgorithm(policy: Union[str, Type[stable_baselines3.common.policies.ActorCriticPolicy]], env: Union[gym.core.Env, stable_baselines3.common.vec_env.base_vec_env.VecEnv, str], learning_rate: Union[float, Callable], n_steps: int, gamma: float, gae_lambda: float, ent_coef: float, vf_coef: float, max_grad_norm: float, use_sde: bool, sde_sample_freq: int, tensorboard_log: Union[str, NoneType] = None, create_eval_env: bool = False, monitor_wrapper: bool = True, policy_kwargs: Union[Dict[str, Any], NoneType] = None, verbose: int = 0, seed: Union[int, NoneType] = None, device: Union[torch.device, str] = 'auto', _init_setup_model: bool = True)\n     |  \n     |  The base for On-Policy algorithms (ex: A2C/PPO).\n     |  \n     |  :param policy: (ActorCriticPolicy or str) The policy model to use (MlpPolicy, CnnPolicy, ...)\n     |  :param env: (Gym environment or str) The environment to learn from (if registered in Gym, can be str)\n     |  :param learning_rate: (float or callable) The learning rate, it can be a function\n     |      of the current progress remaining (from 1 to 0)\n     |  :param n_steps: (int) The number of steps to run for each environment per update\n     |      (i.e. batch size is n_steps * n_env where n_env is number of environment copies running in parallel)\n     |  :param gamma: (float) Discount factor\n     |  :param gae_lambda: (float) Factor for trade-off of bias vs variance for Generalized Advantage Estimator.\n     |      Equivalent to classic advantage when set to 1.\n     |  :param ent_coef: (float) Entropy coefficient for the loss calculation\n     |  :param vf_coef: (float) Value function coefficient for the loss calculation\n     |  :param max_grad_norm: (float) The maximum value for the gradient clipping\n     |  :param use_sde: (bool) Whether to use generalized State Dependent Exploration (gSDE)\n     |      instead of action noise exploration (default: False)\n     |  :param sde_sample_freq: (int) Sample a new noise matrix every n steps when using gSDE\n     |      Default: -1 (only sample at the beginning of the rollout)\n     |  :param tensorboard_log: (str) the log location for tensorboard (if None, no logging)\n     |  :param create_eval_env: (bool) Whether to create a second environment that will be\n     |      used for evaluating the agent periodically. (Only available when passing string for the environment)\n     |  :param monitor_wrapper: When creating an environment, whether to wrap it\n     |      or not in a Monitor wrapper.\n     |  :param policy_kwargs: (dict) additional arguments to be passed to the policy on creation\n     |  :param verbose: (int) the verbosity level: 0 no output, 1 info, 2 debug\n     |  :param seed: (int) Seed for the pseudo random generators\n     |  :param device: (str or th.device) Device (cpu, cuda, ...) on which the code should be run.\n     |      Setting it to auto, the code will be run on the GPU if possible.\n     |  :param _init_setup_model: (bool) Whether or not to build the network at the creation of the instance\n     |  \n     |  Method resolution order:\n     |      OnPolicyAlgorithm\n     |      stable_baselines3.common.base_class.BaseAlgorithm\n     |      abc.ABC\n     |      builtins.object\n     |  \n     |  Methods defined here:\n     |  \n     |  __init__(self, policy: Union[str, Type[stable_baselines3.common.policies.ActorCriticPolicy]], env: Union[gym.core.Env, stable_baselines3.common.vec_env.base_vec_env.VecEnv, str], learning_rate: Union[float, Callable], n_steps: int, gamma: float, gae_lambda: float, ent_coef: float, vf_coef: float, max_grad_norm: float, use_sde: bool, sde_sample_freq: int, tensorboard_log: Union[str, NoneType] = None, create_eval_env: bool = False, monitor_wrapper: bool = True, policy_kwargs: Union[Dict[str, Any], NoneType] = None, verbose: int = 0, seed: Union[int, NoneType] = None, device: Union[torch.device, str] = 'auto', _init_setup_model: bool = True)\n     |      Initialize self.  See help(type(self)) for accurate signature.\n     |  \n     |  collect_rollouts(self, env: stable_baselines3.common.vec_env.base_vec_env.VecEnv, callback: stable_baselines3.common.callbacks.BaseCallback, rollout_buffer: stable_baselines3.common.buffers.RolloutBuffer, n_rollout_steps: int) -> bool\n     |      Collect rollouts using the current policy and fill a `RolloutBuffer`.\n     |      \n     |      :param env: (VecEnv) The training environment\n     |      :param callback: (BaseCallback) Callback that will be called at each step\n     |          (and at the beginning and end of the rollout)\n     |      :param rollout_buffer: (RolloutBuffer) Buffer to fill with rollouts\n     |      :param n_steps: (int) Number of experiences to collect per environment\n     |      :return: (bool) True if function returned with at least `n_rollout_steps`\n     |          collected, False if callback terminated rollout prematurely.\n     |  \n     |  get_torch_variables(self) -> Tuple[List[str], List[str]]\n     |      cf base class\n     |  \n     |  learn(self, total_timesteps: int, callback: Union[NoneType, Callable, List[stable_baselines3.common.callbacks.BaseCallback], stable_baselines3.common.callbacks.BaseCallback] = None, log_interval: int = 1, eval_env: Union[gym.core.Env, stable_baselines3.common.vec_env.base_vec_env.VecEnv, NoneType] = None, eval_freq: int = -1, n_eval_episodes: int = 5, tb_log_name: str = 'OnPolicyAlgorithm', eval_log_path: Union[str, NoneType] = None, reset_num_timesteps: bool = True) -> 'OnPolicyAlgorithm'\n     |      Return a trained model.\n     |      \n     |      :param total_timesteps: (int) The total number of samples (env steps) to train on\n     |      :param callback: (function (dict, dict)) -> boolean function called at every steps with state of the algorithm.\n     |          It takes the local and global variables. If it returns False, training is aborted.\n     |      :param log_interval: (int) The number of timesteps before logging.\n     |      :param tb_log_name: (str) the name of the run for tensorboard log\n     |      :param eval_env: (gym.Env) Environment that will be used to evaluate the agent\n     |      :param eval_freq: (int) Evaluate the agent every ``eval_freq`` timesteps (this may vary a little)\n     |      :param n_eval_episodes: (int) Number of episode to evaluate the agent\n     |      :param eval_log_path: (Optional[str]) Path to a folder where the evaluations will be saved\n     |      :param reset_num_timesteps: (bool) whether or not to reset the current timestep number (used in logging)\n     |      :return: (BaseAlgorithm) the trained model\n     |  \n     |  train(self) -> None\n     |      Consume current rollout data and update policy parameters.\n     |      Implemented by individual algorithms.\n     |  \n     |  ----------------------------------------------------------------------\n     |  Data and other attributes defined here:\n     |  \n     |  __abstractmethods__ = frozenset()\n     |  \n     |  ----------------------------------------------------------------------\n     |  Methods inherited from stable_baselines3.common.base_class.BaseAlgorithm:\n     |  \n     |  excluded_save_params(self) -> List[str]\n     |      Returns the names of the parameters that should be excluded by default\n     |      when saving the model.\n     |      \n     |      :return: ([str]) List of parameters that should be excluded from save\n     |  \n     |  get_env(self) -> Union[stable_baselines3.common.vec_env.base_vec_env.VecEnv, NoneType]\n     |      Returns the current environment (can be None if not defined).\n     |      \n     |      :return: (Optional[VecEnv]) The current environment\n     |  \n     |  get_vec_normalize_env(self) -> Union[stable_baselines3.common.vec_env.vec_normalize.VecNormalize, NoneType]\n     |      Return the ``VecNormalize`` wrapper of the training env\n     |      if it exists.\n     |      :return: Optional[VecNormalize] The ``VecNormalize`` env.\n     |  \n     |  predict(self, observation: numpy.ndarray, state: Union[numpy.ndarray, NoneType] = None, mask: Union[numpy.ndarray, NoneType] = None, deterministic: bool = False) -> Tuple[numpy.ndarray, Union[numpy.ndarray, NoneType]]\n     |      Get the model's action(s) from an observation\n     |      \n     |      :param observation: (np.ndarray) the input observation\n     |      :param state: (Optional[np.ndarray]) The last states (can be None, used in recurrent policies)\n     |      :param mask: (Optional[np.ndarray]) The last masks (can be None, used in recurrent policies)\n     |      :param deterministic: (bool) Whether or not to return deterministic actions.\n     |      :return: (Tuple[np.ndarray, Optional[np.ndarray]]) the model's action and the next state\n     |          (used in recurrent policies)\n     |  \n     |  save(self, path: str, exclude: Union[List[str], NoneType] = None, include: Union[List[str], NoneType] = None) -> None\n     |      Save all the attributes of the object and the model parameters in a zip-file.\n     |      \n     |      :param path: path to the file where the rl agent should be saved\n     |      :param exclude: name of parameters that should be excluded in addition to the default one\n     |      :param include: name of parameters that might be excluded but should be included anyway\n     |  \n     |  set_env(self, env: Union[gym.core.Env, stable_baselines3.common.vec_env.base_vec_env.VecEnv]) -> None\n     |      Checks the validity of the environment, and if it is coherent, set it as the current environment.\n     |      Furthermore wrap any non vectorized env into a vectorized\n     |      checked parameters:\n     |      - observation_space\n     |      - action_space\n     |      \n     |      :param env: The environment for learning a policy\n     |  \n     |  set_random_seed(self, seed: Union[int, NoneType] = None) -> None\n     |      Set the seed of the pseudo-random generators\n     |      (python, numpy, pytorch, gym, action_space)\n     |      \n     |      :param seed: (int)\n     |  \n     |  ----------------------------------------------------------------------\n     |  Class methods inherited from stable_baselines3.common.base_class.BaseAlgorithm:\n     |  \n     |  load(load_path: str, env: Union[gym.core.Env, stable_baselines3.common.vec_env.base_vec_env.VecEnv, NoneType] = None, **kwargs) from abc.ABCMeta\n     |      Load the model from a zip-file\n     |      \n     |      :param load_path: the location of the saved data\n     |      :param env: the new environment to run the loaded model on\n     |          (can be None if you only need prediction from a trained model) has priority over any saved environment\n     |      :param kwargs: extra arguments to change the model when loading\n     |  \n     |  ----------------------------------------------------------------------\n     |  Data descriptors inherited from stable_baselines3.common.base_class.BaseAlgorithm:\n     |  \n     |  __dict__\n     |      dictionary for instance variables (if defined)\n     |  \n     |  __weakref__\n     |      list of weak references to the object (if defined)\n\nDATA\n    Any = typing.Any\n    Callable = typing.Callable\n    Dict = typing.Dict\n    GymEnv = typing.Union[gym.core.Env, stable_baselines3.common.vec_env.b...\n    List = typing.List\n    MaybeCallback = typing.Union[NoneType, typing.Callable, typing.L... st...\n    Optional = typing.Optional\n    Tuple = typing.Tuple\n    Type = typing.Type\n    Union = typing.Union\n\nFILE\n    /home/darijan/.local/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py\n\n\n"
    }
   ],
   "source": [
    "import stable_baselines3\n",
    "help(stable_baselines3)\n",
    "help(stable_baselines3.common.on_policy_algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def maml_grad(model, inputs, outputs, lr, batch=1):\n",
    "    \"\"\"\n",
    "    Update a model's gradient using MAML.\n",
    "    The gradient will point in the direction that\n",
    "    improves the total loss across all inner-loop\n",
    "    mini-batches.\n",
    "    \n",
    "    Args:\n",
    "        model: an nn.Module for training.\n",
    "        inputs: a large batch of model inputs.\n",
    "        outputs: a large batch of model outputs.\n",
    "        lr: the inner-loop SGD learning rate.\n",
    "        batch: the inner-loop batch size.\n",
    "    \"\"\"\n",
    "    params = list(model.parameters())\n",
    "    device = params[0].device\n",
    "    initial_values = []\n",
    "    final_values = []\n",
    "    losses = []\n",
    "    scalar_losses = []\n",
    "\n",
    "    for i in range(0, inputs.shape[0], batch):\n",
    "        x = inputs[i:i+batch]\n",
    "        y = outputs[i:i+batch]\n",
    "        target = y.to(device)\n",
    "        out = model(x.to(device))\n",
    "\n",
    "        loss = -(outputs*torch.log(out+1e-8)).mean()\n",
    "        losses.append(loss)\n",
    "        scalar_losses.append(loss.item())\n",
    "        initial_values.append([p.clone().detach() for p in params])\n",
    "        updated = []\n",
    "        grads = torch.autograd.grad(loss, params, create_graph=True, retain_graph=True)\n",
    "        for grad, param in zip(grads, params):\n",
    "            x = param - lr * grad\n",
    "            updated.append(x)\n",
    "            param.data.copy_(x)\n",
    "        final_values.append(updated)\n",
    "\n",
    "    gradient = [torch.zeros_like(p) for p in params]\n",
    "    for loss, initial, final in list(zip(losses, initial_values, final_values))[::-1]:\n",
    "        for p, x in zip(params, initial):\n",
    "            p.data.copy_(x)\n",
    "        grad1 = torch.autograd.grad(loss, params, retain_graph=True)\n",
    "        grad2 = torch.autograd.grad(final, params, grad_outputs=gradient, retain_graph=True)\n",
    "        gradient = [v1 + v2 for v1, v2 in zip(grad1, grad2)]\n",
    "\n",
    "    for p, g in zip(params, gradient):\n",
    "        if p.grad is None:\n",
    "            p.grad = g\n",
    "        else:\n",
    "            p.grad.add_(g)\n",
    "            \n",
    "    return scalar_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mymaml_grad(model, inputs_train, targets_train, inputs_test, targets_test, lr):\n",
    "    params = list(model.parameters())\n",
    "    import copy\n",
    "    data_of_params = copy.deepcopy([p.data for p in params])\n",
    "    batch_size = len(inputs_train)\n",
    "    test_scalar_losses = []\n",
    "\n",
    "    for x_train, y_train, x_test, y_test in zip(inputs_train, targets_train, inputs_test, targets_test):\n",
    "        preds = model(x_train)\n",
    "        loss = -(y_train*torch.log(preds+1e-8)).mean()\n",
    "        grads = torch.autograd.grad(loss, params, create_graph=True, retain_graph=True)\n",
    "        for param, grad in zip(params, grads):\n",
    "            x = param -lr*grad\n",
    "            param.data.copy_(x)\n",
    "        preds = model(x_test)\n",
    "        loss = -(y_test*torch.log(preds+1e-8)).mean()/batch_size\n",
    "        test_scalar_losses.append(loss.item())\n",
    "        loss.backward()\n",
    "\n",
    "        for param, data in zip(params, data_of_params):\n",
    "            param.data = data\n",
    "\n",
    "    return test_scalar_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mymaml_grad(model, intrain, targtrain, intest, targtest, lr):\n",
    "    params = list(model.parameters())\n",
    "    batch_size = len(intrain)\n",
    "    test_scalar_losses = []\n",
    "\n",
    "    for x_train, y_train, x_test, y_test in zip(intrain, targtrain, intest, targtest):\n",
    "\n",
    "        preds = model(x_train)\n",
    "        loss = -(y_train*torch.log(preds+1e-8)).mean()\n",
    "        # grads = torch.autograd.grad(loss, params, create_graph=True, retain_graph=True)\n",
    "        loss.backward()\n",
    "\n",
    "        cmodel = copy.deepcopy(model)\n",
    "        cparams = list(cmodel.parameters())\n",
    "        for param, cparam, grad in zip(params, cparams, grads):\n",
    "            tmp = param - lr*grad \n",
    "            with torch.no_grad():\n",
    "                cparam.copy_( tmp )\n",
    "\n",
    "        for name, p in model.named_parameters():\n",
    "            p2 = torch.nn.Parameter( p + 10 )\n",
    "            setattr_rec(model, name, p2)\n",
    "\n",
    "        preds = cmodel(x_test)\n",
    "        loss = -(y_test*torch.log(preds+1e-8)).mean()/batch_size\n",
    "        test_scalar_losses.append(loss.item())\n",
    "        # grads1 = torch.autograd.grad(loss, cparams)\n",
    "        # grads2 = torch.autograd.grad(new,   params)\n",
    "        # grads = [g1*g2 for g1, g2 in zip(grads1, grads2)]\n",
    "        grads = torch.autograd.grad(loss, params, allow_unused=True)\n",
    "\n",
    "        print(grads[0])#, grads1[0], grads2[0])\n",
    "        for param, grad in zip(params, grads):\n",
    "            if param.grad is None:\n",
    "                param.grad = grad\n",
    "            else:\n",
    "                param.grad += grad\n",
    "    return test_scalar_losses"
   ]
  }
 ]
}